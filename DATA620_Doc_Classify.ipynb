{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Document Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to be able to classify new \"test\" documents using already classified \"training\" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  Here is one example of such data: http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "For this project, you can either use the above dataset to predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Corpus**\n",
    "\n",
    "For this project I am using the spam and ham data available here : http://spamassassin.apache.org/old/publiccorpus/ \n",
    "\n",
    "I use the following corpora:\n",
    "\n",
    "    To train:\n",
    "        \n",
    "        20021010_easy_ham.tar.bz2\n",
    "        20021010_spam.tar.bz2\n",
    "        \n",
    "    To test:\n",
    "    \n",
    "        20030228_easy_ham.tar.bz2\n",
    "        20030228_spam.tar.bz2\n",
    "        \n",
    "For the following process I have unzipped the above data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Corpora**\n",
    "\n",
    "The following function processes the files present in the directories and returns 3 outputs PlaintextCorpusReader, sentencces in the corpus in tokenized form and the emails with spam and ham tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(dir, tag):\n",
    "    \n",
    "    plain_corpus = PlaintextCorpusReader(dir, '.*', encoding='latin-1') \n",
    "    \n",
    "    sen_corpus  = nltk.Text(plain_corpus.sents())\n",
    "    \n",
    "    email_tag_corpus = [(e, tag) for e in sen_corpus]\n",
    "    \n",
    "    return plain_corpus, sen_corpus, email_tag_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ham train dataset is loaded from the local directory\n",
    "\n",
    "The above function is called to get the plaintext, sentence and email tag of ham corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ham\n",
    "ham_dir = './20021010_easy_ham.tar/easy_ham/' \n",
    "\n",
    "plain_ham, sen_ham, email_tag_ham = process_corpus(ham_dir, 'ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam train dataset is loaded from the local directory\n",
    "\n",
    "The above function is called to get the plaintext, sentence and email tag of spam corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spam\n",
    "spam_dir = './20021010_spam.tar/spam/'\n",
    "\n",
    "plain_spam, sen_spam, email_tag_spam = process_corpus(spam_dir, 'spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function displays the file count, word count and sentence count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_details(corpus):\n",
    "\n",
    "    fileid = len(corpus.fileids())\n",
    "    wordcount = len(corpus.words())\n",
    "    sencount = len(corpus.sents())\n",
    "\n",
    "    print(\"No. Of Files = {}\".format(fileid))    \n",
    "    print(\"No. Of Words = {}\".format(wordcount))\n",
    "    print(\"No. Of Sentences = {}\".format(sencount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAM DATA SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Of Files = 501\n",
      "No. Of Words = 769207\n",
      "No. Of Sentences = 12107\n"
     ]
    }
   ],
   "source": [
    "display_details(plain_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPAM DATA SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Of Files = 2552\n",
      "No. Of Words = 2449010\n",
      "No. Of Sentences = 39959\n"
     ]
    }
   ],
   "source": [
    "display_details(plain_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining both corpora and shuffling**\n",
    "\n",
    "The email with the corresponding tags obtained above of both spam and ham are combined and shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "email_tag = email_tag_spam + email_tag_ham\n",
    "\n",
    "random.shuffle(email_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature**\n",
    "\n",
    "Here I use the Lemmatizer feature.\n",
    "\n",
    "The feature is extracted into *ext_feature*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordNetLemmatizer\n",
    "def feature1(s):\n",
    "    w = WordNetLemmatizer()\n",
    "    return [w.lemmatize(wrd.lower()) for wrd in s]\n",
    "\n",
    "stopwrd = stopwords.words(\"english\")\n",
    "\n",
    "def get_features(e, f):\n",
    "    return {wrd: True for wrd in f(e) if not wrd in stopwrd}\n",
    "\n",
    "ext_feature = [(get_features(e, feature1), l) for (e, l) in email_tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Classifier**\n",
    "\n",
    "The following function impliments Naive Bayes Classifier on a portion of the corpus given as argument to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive(f, p):\n",
    "    l = int(len(f) * p)\n",
    "    train, test = f[:l], f[l:]\n",
    "    \n",
    "    NB = NaiveBayesClassifier.train(train)\n",
    "    return train, test, NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using 3 sets of train and test sets to classify the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1, NB1 = Naive(ext_feature, 0.5)\n",
    "\n",
    "train2, test2, NB2 = Naive(ext_feature, 0.7)\n",
    "\n",
    "train3, test3, NB3 = Naive(ext_feature, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function prints the accuracy of the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyfn(train,test,classifier,f):\n",
    "\tx=nltk.classify.accuracy(classifier, train)\n",
    "\tprint(\"train accuracy\",x)\n",
    "\n",
    "\ty=nltk.classify.accuracy(classifier, test)\n",
    "\tprint(\"Performance on the test set for feature 1:\",y)\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9333154073675719\n",
      "Performance on the test set for feature 1: 0.9119578995889832\n"
     ]
    }
   ],
   "source": [
    "accuracyfn(train1, test1, NB1,feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9333260165724634\n",
      "Performance on the test set for feature 1: 0.9185019206145967\n"
     ]
    }
   ],
   "source": [
    "accuracyfn(train2, test2, NB2,feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9357220597964105\n",
      "Performance on the test set for feature 1: 0.9330985915492958\n"
     ]
    }
   ],
   "source": [
    "accuracyfn(train3, test2, NB3,feature1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above it is clear that set 3 is the best set so I am using that to classify the test corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 36597, 'spam': 1979})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ham 20030228\n",
    "ham2003_dir = './20030228_easy_ham.tar/easy_ham/'\n",
    "plain_ham2003, sen_ham2003, email_tag_ham2003 = process_corpus(ham2003_dir, 'ham')\n",
    "\n",
    "predict_ham = [NB3.classify(get_features(e, feature1)) for (e, l) in email_tag_ham2003]\n",
    "\n",
    "Counter(predict_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38576"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'spam': 10508, 'ham': 1518})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Spam 20030228\n",
    "spam2003_dir = './20030228_spam.tar/spam/'\n",
    "plain_spam2003, sen_spam2003, email_tag_spam2003 = process_corpus(spam2003_dir, 'spam')\n",
    "\n",
    "\n",
    "predict_spam = [NB3.classify(get_features(e, feature1)) for (e, l) in email_tag_spam2003]\n",
    "\n",
    "Counter(predict_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12026"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "From the above predictions it is clear that almost **95%** of Ham mails were correctly identified and around **87%** Spam mails were correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Video**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/qgtDZnZJcT0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
